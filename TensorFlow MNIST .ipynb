{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcacf1ae",
   "metadata": {},
   "source": [
    "Deep Neural Network for MNIST Classification\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. it provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n",
    "  \n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "Fares Yassen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27644c64",
   "metadata": {},
   "source": [
    "# Importing the relevant packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71727a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e27210",
   "metadata": {},
   "source": [
    "# Data\n",
    "We will load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3424597",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist',with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "#shuffling is important to do in preprocessing \n",
    "#dividing the data into buffers to optimize the computational power \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "#We will take validation data as 10% of the training data, as we already calculated\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "#the rest is train data so we skip the number of validation data samples\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "#we dont really need batching for validation data as we are only forward propgating so that takes not much computational power\n",
    "# However, the model excepts validation data in a batch form too , so one single batch is enough \n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# convert the validation data into a input-target shape like test and train datsets \n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c0ea1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f391f5",
   "metadata": {},
   "source": [
    "## outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "faf2bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 280\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            #input layer\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            #hidden layer 1\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            #hidden layer 2\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            #output layer\n",
    "                            # note that we used softmax since we're making a classifier we need output values as probabilities \n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')    \n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c94982",
   "metadata": {},
   "source": [
    "## Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ca47ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998321e",
   "metadata": {},
   "source": [
    "## Training\n",
    "Here we train the model we have built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d2eec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 5s - loss: 0.2490 - accuracy: 0.9273 - val_loss: 0.1317 - val_accuracy: 0.9632 - 5s/epoch - 10ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 0.0948 - accuracy: 0.9713 - val_loss: 0.0846 - val_accuracy: 0.9752 - 6s/epoch - 10ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 5s - loss: 0.0621 - accuracy: 0.9808 - val_loss: 0.0634 - val_accuracy: 0.9820 - 5s/epoch - 9ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 5s - loss: 0.0450 - accuracy: 0.9861 - val_loss: 0.0527 - val_accuracy: 0.9842 - 5s/epoch - 9ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 0.0337 - accuracy: 0.9889 - val_loss: 0.0437 - val_accuracy: 0.9882 - 6s/epoch - 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1453f3730a0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daccbb8a",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34f3c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 676ms/step - loss: 0.0711 - accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4bde9933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07. Test accuracy: 97.99%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
